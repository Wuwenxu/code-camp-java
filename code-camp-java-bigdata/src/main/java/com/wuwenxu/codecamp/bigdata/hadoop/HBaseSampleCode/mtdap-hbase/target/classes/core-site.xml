<?xml version="1.0" encoding="UTF-8"?><configuration>
<property>
<name>hadoop.proxyuser.mapred.hosts</name>
<value>*</value>
</property>
<property>
<name>fs.nas.task.nodes</name>
<value></value>
</property>
<property>
<name>cas.conf.serverName.on</name>
<value>https://192.168.94.48:20026/HDFS/NameNode/63</value>
</property>
<property>
<name>hadoop.security.auth_to_local</name>
<value>RULE:[1:$1]
RULE:[2:$1]
DEFAULT</value>
</property>
<property>
<name>hadoop.proxyuser.ftpserver.hosts</name>
<value>*</value>
</property>
<property>
<name>ipc.ping.interval</name>
<value>60000</value>
</property>
<property>
<name>hadoop.proxyuser.impala.hosts</name>
<value>*</value>
</property>
<property>
<name>oi.dfs.colocation.zookeeper.quorum</name>
<value>guoshuai3:24002,guoshuai2:24002,guoshuai1:24002</value>
</property>
<property>
<name>hadoop.proxyuser.HTTP.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark3.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark2.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark4.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.loader.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hbase1.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hbase3.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.mapred.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.security.logout.ui.enable</name>
<value>false</value>
</property>
<property>
<name>hadoop.security.group.mapping</name>
<value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
</property>
<property>
<name>hadoop.ssl.keystores.factory.class</name>
<value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
</property>
<property>
<name>ha.zookeeper.quorum</name>
<value>guoshuai3:24002,guoshuai2:24002,guoshuai1:24002</value>
</property>
<property>
<name>hadoop.http.filter.initializers</name>
<value>com.huawei.hadoop.adapter.sso.FlowCtrlFilter,com.huawei.hadoop.adapter.sso.XSSFilterInitializer</value>
</property>
<property>
<name>hadoop.proxyuser.miner.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.miner.hosts</name>
<value>*</value>
</property>
<property>
<name>sas.plugin.enable</name>
<value>false</value>
</property>
<property>
<name>hadoop.proxyuser.omm.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.ssl.enabled.protocols</name>
<value>TLSv1.1,TLSv1.2</value>
</property>
<property>
<name>hadoop.proxyuser.spark1.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hdfs.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hbase3.hosts</name>
<value>*</value>
</property>
<property>
<name>fs.trash.interval</name>
<value>1440</value>
</property>
<property>
<name>hadoop.proxyuser.hive4.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.http.authentication.center.listener</name>
<value>org.jasig.cas.client.session.SingleSignOutHttpSessionListener</value>
</property>
<property>
<name>hadoop.proxyuser.hive2.hosts</name>
<value>*</value>
</property>
<property>
<name>clientPort</name>
<value>24002</value>
</property>
<property>
<name>ipc.client.rpc.timeout</name>
<value>300000</value>
</property>
<property>
<name>fs.viewfs.mounttable.ClusterX.homedir</name>
<value>/</value>
</property>
<property>
<name>hadoop.proxyuser.hive1.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hbase.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hive.hosts</name>
<value>*</value>
</property>
<property>
<name>fs.client.resolve.topology.enabled</name>
<value>true</value>
</property>
<property>
<name>hadoop.spnego.allowed.ips</name>
<value>.*</value>
</property>
<property>
<name>hadoop.security.instrumentation.requires.admin</name>
<value>true</value>
</property>
<property>
<name>ipc.client.ping</name>
<value>true</value>
</property>
<property>
<name>hadoop.proxyuser.hbase4.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.oozie.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hive1.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/srv/BigData</value>
</property>
<property>
<name>fs.nas.impl</name>
<value>com.huawei.nasfilesystem.ShareNASFileSystem</value>
</property>
<property>
<name>hadoop.proxyuser.hbase2.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark2.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.oozie.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark1.groups</name>
<value>*</value>
</property>
<property>
<name>cas.conf.serverName.off</name>
<value>http://guoshuai2:25002</value>
</property>
<property>
<name>hadoop.proxyuser.solr.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.http.server.MaxRequests</name>
<value>2000</value>
</property>
<property>
<name>hadoop.ssl.require.client.cert</name>
<value>false</value>
</property>
<property>
<name>cas.conf.casServerUrlPrefix.on</name>
<value>https://192.168.94.47:20027/cas/</value>
</property>
<property>
<name>fs.trash.checkpoint.interval</name>
<value>60</value>
</property>
<property>
<name>hadoop.security.authorization</name>
<value>false</value>
</property>
<property>
<name>hadoop.proxyuser.solr.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.loader.hosts</name>
<value>*</value>
</property>
<property>
<name>ipc.client.connect.timeout</name>
<value>20000</value>
</property>
<property>
<name>cas.conf.casServerUrlPrefix.off</name>
<value>https://192.168.94.48:20009/cas/</value>
</property>
<property>
<name>hadoop.security.authentication</name>
<value>simple</value>
</property>
<property>
<name>hadoop.proxyuser.hdfs.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark4.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hue.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hbase1.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.security.crypto.implementation.class</name>
<value>com.huawei.hadoop.adapter.security.HadoopCryptoAdapter</value>
</property>
<property>
<name>hadoop.proxyuser.hue.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hbase4.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.ftpserver.groups</name>
<value>*</value>
</property>
<property>
<name>io.compression.codecs</name>
<value>org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.Lz4Codec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.GzipCodec</value>
</property>
<property>
<name>hadoop.proxyuser.hive.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hive3.hosts</name>
<value>*</value>
</property>
<property>
<name>fs.defaultFS</name>
<value>hdfs://hacluster</value>
</property>
<property>
<name>hadoop.proxyuser.impala.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.ssl.client.conf</name>
<value>ssl-client.xml</value>
</property>
<property>
<name>fs.nas.mount.dir</name>
<value>/mnt/nfsdata0</value>
</property>
<property>
<name>ipc.maximum.data.length</name>
<value>268435456</value>
</property>
<property>
<name>fs.AbstractFileSystem.nas.impl</name>
<value>com.huawei.nasfilesystem.WushanFs</value>
</property>
<property>
<name>cas.proxy.switch</name>
<value>true</value>
</property>
<property>
<name>hadoop.ssl.hostname.verifier</name>
<value>ALLOW_ALL</value>
</property>
<property>
<name>hadoop.http.tolerance.time</name>
<value>300000</value>
</property>
<property>
<name>hadoop.proxyuser.hive3.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.ssl.server.conf</name>
<value>ssl-server.xml</value>
</property>
<property>
<name>hadoop.proxyuser.hive2.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hbase.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.spark3.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hive4.hosts</name>
<value>*</value>
</property>
<property>
<name>ipc.client.fallback-to-simple-auth-allowed</name>
<value>true</value>
</property>
<property>
<name>hadoop.proxyuser.hbase2.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.omm.hosts</name>
<value>*</value>
</property>
<property>
<name>cas.conf.casServerLoginUrl</name>
<value>https://192.168.94.48:20009/cas/login</value>
</property>
<property>
<name>hadoop.http.server.name</name>
<value>http://guoshuai2:25002</value>
</property>
<property>
<name>ha.zookeeper.session-timeout.ms</name>
<value>45000</value>
</property>
<property>
<name>fs.s3a.connection.ssl.enabled</name>
<value>false</value>
</property>
<property>
<name>hadoop.rpc.protection</name>
<value>authentication</value>
</property>
<property>
<name>ha.health-monitor.rpc-timeout.ms</name>
<value>180000</value>
</property>
<property>
<name>hadoop.http.staticuser.user</name>
<value>hdfs</value>
</property>
<property>
<name>fs.s3a.endpoint</name>
<value></value>
</property>
<property>
<name>hadoop.proxyuser.HTTP.groups</name>
<value>*</value>
</property>
</configuration>
